# 评估指标对比分析

## 一、evaluate_copa_scores.py 的评估指标

### 1.1 评估流程

```
原始预测值/标签 → 范式聚合(求和) → Z分数标准化 → T分数转换 → 等级划分 → 范围匹配
```

### 1.2 核心特点

**基于心理学测试的标准化评估：**

1. **范式聚合** (第102-103行)
   ```python
   pred_xj = group_df.loc[indices, "pred_class"].sum()  # 对每个范式的多个题目求和
   label_xj = group_df.loc[indices, "label"].sum()
   ```
   - 将每个范式（P1-P12）内的多个题目分数求和
   - 例如：P1包含8个题目，将这8个题目的分数相加

2. **Z分数标准化** (第106-107行)
   ```python
   pred_Z = (pred_xj - norm["mean"]) / norm["std"]
   label_Z = (label_xj - norm["mean"]) / norm["std"]
   ```
   - 使用**常模数据**（norm_data）进行标准化
   - 常模数据包含不同群体（i1男犯/i2女犯/i3未成年）的均值和标准差
   - 将原始分数转换为标准正态分布

3. **T分数转换** (第109-110行)
   ```python
   pred_T = int(50 + 10 * pred_Z)  # T分数 = 50 + 10 * Z分数
   label_T = int(50 + 10 * label_Z)
   ```
   - T分数：均值50，标准差10的标准分数
   - 范围通常在20-80之间

4. **等级划分** (第44-49行)
   ```python
   t_ranges = {
       "低分": {"min": 0, "max": 35},
       "较低分": {"min": 36, "max": 45},
       "中等分": {"min": 46, "max": 54},
       "较高": {"min": 55, "max": 64},
       "高分": {"min": 65, "max": 100}
   }
   ```
   - 将T分数映射到5个等级

5. **准确率计算** (第116行)
   ```python
   in_range = t_ranges[label_level]["min"] <= pred_T <= t_ranges[label_level]["max"]
   ```
   - **准确率定义**：预测的T分数是否落在真实标签对应的等级范围内
   - 这是一个**范围匹配**的评估方式，不是精确匹配

### 1.3 评估特点

- ✅ **考虑群体差异**：使用不同群体的常模数据
- ✅ **标准化评估**：通过T分数消除不同范式间的量纲差异
- ✅ **等级评估**：关注等级范围而非精确值
- ✅ **聚合评估**：在范式级别评估，而非单个题目

---

## 二、MMSA项目的评估指标

### 2.1 回归任务指标（当前项目使用）

**主要指标：**

1. **MAE (Mean Absolute Error)** - 平均绝对误差
   ```python
   mae = np.mean(np.absolute(test_preds - test_truth))
   ```
   - 直接计算预测值与真实值的平均绝对差异
   - 值越小越好，范围通常[0, +∞)

2. **Corr (Correlation Coefficient)** - 相关系数
   ```python
   corr = np.corrcoef(test_preds, test_truth)[0][1]
   ```
   - 衡量预测值与真实值的线性相关程度
   - 范围[-1, 1]，越接近1越好

3. **Mult_acc_5 / Mult_acc_7** - 多分类准确率
   ```python
   # 将回归值映射到5类或7类
   test_preds_a5 = np.clip(test_preds, a_min=-2., a_max=2.)
   mult_a5 = self.__multiclass_acc(test_preds_a5, test_truth_a5)
   ```
   - 将连续值映射到离散类别后计算准确率

4. **Has0_acc_2 / Non0_acc_2** - 二分类准确率
   ```python
   # Has0: 包含0值（>=0 vs <0）
   binary_truth = (test_truth >= 0)
   binary_preds = (test_preds >= 0)
   acc2 = accuracy_score(binary_preds, binary_truth)
   
   # Non0: 排除0值（>0 vs <0）
   non_zeros = np.array([i for i, e in enumerate(test_truth) if e != 0])
   non_zeros_binary_truth = (test_truth[non_zeros] > 0)
   non_zeros_binary_preds = (test_preds[non_zeros] > 0)
   ```

5. **F1_score** - F1分数
   - 精确率和召回率的调和平均

### 2.2 评估特点

- ✅ **直接评估**：直接比较预测值和真实值
- ✅ **样本级评估**：每个样本独立评估
- ✅ **精确匹配**：关注预测的精确性
- ✅ **通用指标**：适用于一般的情感分析任务

---

## 三、核心区别对比

| 维度 | evaluate_copa_scores.py | MMSA项目指标 |
|------|------------------------|-------------|
| **评估对象** | 范式级别的聚合分数 | 单个样本的预测值 |
| **标准化** | 使用常模数据（群体特定） | 无标准化或数据级标准化 |
| **评估粒度** | 等级范围匹配 | 精确值或类别匹配 |
| **准确率定义** | T分数是否在标签等级范围内 | 预测值是否等于真实值（或类别） |
| **适用场景** | 心理学测试评估（COPA） | 通用情感分析任务 |
| **群体差异** | 考虑（i1/i2/i3不同常模） | 不考虑 |
| **聚合方式** | 范式内题目求和 | 无聚合，样本独立 |

---

## 四、具体示例

### 示例1：evaluate_copa_scores.py

假设：
- 真实标签等级：**"中等分"** (T分数范围46-54)
- 预测T分数：**50**
- 评估结果：✅ **正确**（因为50在46-54范围内）

即使：
- 真实T分数：**48**
- 预测T分数：**52**
- 虽然相差4分，但仍在同一等级范围内，所以算正确

### 示例2：MMSA项目指标

假设：
- 真实值：**0.5**
- 预测值：**0.7**
- MAE = |0.7 - 0.5| = **0.2**
- 如果映射到二分类（>=0为正，<0为负）：
  - 真实类别：**正**（0.5 >= 0）
  - 预测类别：**正**（0.7 >= 0）
  - Has0_acc_2 = **1.0**（正确）

---

## 五、为什么会有这种区别？

### 5.1 evaluate_copa_scores.py的设计原因

1. **心理学测试特性**
   - 心理学测试通常关注**等级评估**而非精确分数
   - 不同群体的常模不同，需要标准化

2. **临床实用性**
   - 临床应用中，等级评估（如"中等分"）比精确分数更有意义
   - 允许一定的误差范围，更符合实际应用

3. **范式评估**
   - COPA测试有12个范式，每个范式包含多个题目
   - 需要在范式级别评估，而非单个题目

### 5.2 MMSA项目指标的设计原因

1. **通用性**
   - 适用于多种情感分析任务（MOSI、MOSEI、SIMS等）
   - 不针对特定应用场景

2. **精确性**
   - 关注模型的预测精确度
   - 适合模型性能比较和优化

3. **标准化评估**
   - 使用通用的机器学习评估指标
   - 便于与其他研究对比

---

## 六、总结

**evaluate_copa_scores.py** 是一个**领域特定的评估系统**，专门为心理学测试（COPA）设计，特点是：
- 使用常模数据标准化
- 等级范围匹配
- 范式级别聚合评估

**MMSA项目指标** 是一个**通用的评估系统**，适用于一般的情感分析任务，特点是：
- 直接比较预测值和真实值
- 精确匹配或类别匹配
- 样本级别独立评估

两者服务于不同的应用场景和评估需求。

